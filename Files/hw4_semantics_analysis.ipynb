{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfcd9dc9-170f-4d0a-9d02-a883aec20f6d",
   "metadata": {},
   "source": [
    "# <center> HW4 - LSA based semantics analysis </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d58a4d-f8dd-4b1f-932e-50bca46a6f4e",
   "metadata": {},
   "source": [
    "&copy; 2023 Kaiwen Zhou"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaf6ddb-e265-41b5-86c5-9030bec4fc1e",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b40bc852-8b31-45c8-a559-f5bf8369a8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0740df53-08c2-4998-a384-b27ae545e313",
   "metadata": {},
   "source": [
    "# (a) Download Dataset, Remove URLs then Save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8e1c082-8369-40a9-8d62-dba92babeb11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    $BYND - JPMorgan reels in expectations on Beyo...\n",
      "1    $CCL $RCL - Nomura points to bookings weakness...\n",
      "2    $CX - Cemex cut at Credit Suisse, J.P. Morgan ...\n",
      "3    $ESS: BTIG Research cuts to Neutral https://t....\n",
      "4    $FNKO - Funko slides after Piper Jaffray PT cu...\n",
      "5    $FTI - TechnipFMC downgraded at Berenberg but ...\n",
      "6        $GM - GM loses a bull https://t.co/tdUfG5HbXy\n",
      "7    $GM: Deutsche Bank cuts to Hold https://t.co/7...\n",
      "8                   $GTT: Cowen cuts to Market Perform\n",
      "9    $HNHAF $HNHPD $AAPL - Trendforce cuts iPhone e...\n",
      "Name: text, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3k/bm7vtjxd1c35xlw7l7qpjz3r0000gn/T/ipykernel_26509/3951130839.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text'][i] = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', text, flags=re.MULTILINE)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment/raw/main/sent_train.csv\")\n",
    "print(df['text'][:10])\n",
    "for i, text, in enumerate(df['text']):\n",
    "    df['text'][i] = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', text, flags=re.MULTILINE)\n",
    "df.to_csv(\"sent_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18667fb-d0db-4794-92ae-e104b3b8fff9",
   "metadata": {},
   "source": [
    "check if they look nice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32f9120f-da94-4578-b176-5a593ffe8e0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    $BYND - JPMorgan reels in expectations on Beyo...\n",
       "1    $CCL $RCL - Nomura points to bookings weakness...\n",
       "2    $CX - Cemex cut at Credit Suisse, J.P. Morgan ...\n",
       "3                 $ESS: BTIG Research cuts to Neutral \n",
       "4     $FNKO - Funko slides after Piper Jaffray PT cut \n",
       "5    $FTI - TechnipFMC downgraded at Berenberg but ...\n",
       "6                               $GM - GM loses a bull \n",
       "7                     $GM: Deutsche Bank cuts to Hold \n",
       "8                   $GTT: Cowen cuts to Market Perform\n",
       "9    $HNHAF $HNHPD $AAPL - Trendforce cuts iPhone e...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d47f2a2-f3af-4330-b559-917d3fefd4a9",
   "metadata": {},
   "source": [
    "**Looks good.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9793e334-fe42-4738-b852-eef4f2522156",
   "metadata": {},
   "source": [
    "# (b) Create a doc2vec(doc, tfidf_vectorizer) function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "034a0b62-c1c3-4458-9fda-f0f018968db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the TfidfVectorizer to fit the doc and generate \n",
    "# vectorized outcome and the feature vector\n",
    "Tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_df=0.5, # ignore terms which occur in more than half of the documents\n",
    "    max_features=10000,\n",
    "    min_df=2, # ignore terms which occur in less than 2 documents\n",
    "    stop_words='english',\n",
    "    norm='l2',\n",
    "    use_idf=True, \n",
    "    analyzer='word',\n",
    "    token_pattern = '(?u)\\\\b[a-zA-Z]\\\\w+\\\\b'\n",
    ")\n",
    "\n",
    "# Setting up the CountVectorizer to count the # of each feature for each doc\n",
    "count_vectorizer = CountVectorizer(max_df=0.5, # ignore terms which occur in more than half of the documents\n",
    "                                   max_features=10000,\n",
    "                                   min_df=2, # ignore terms which occur in less than 2 documents\n",
    "                                   stop_words='english',\n",
    "                                   analyzer='word',\n",
    "                                   token_pattern = '(?u)\\\\b[a-zA-Z]\\\\w+\\\\b'\n",
    "            )\n",
    "\n",
    "# vectorize the given doc\n",
    "def docs2vec(docs, tfidf_vectorizer):\n",
    "    vec = tfidf_vectorizer.fit_transform(docs).toarray()\n",
    "    doc_features = tfidf_vectorizer.get_feature_names()\n",
    "    doc_counts = count_vectorizer.fit_transform(docs).toarray()\n",
    "    return vec, doc_features, doc_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65ebd089-a8ad-42de-bb66-631e90666b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhoukaiwen/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# tidy up the corpus\n",
    "full_corpus = list(df['text'])\n",
    "\n",
    "# Train tfidf_vectorizer on the corpus (full dataset).\n",
    "full_corpus_tfidf, full_corpus_feature_names, full_corpus_feature_count = docs2vec(full_corpus, Tfidf_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beef132-7e82-4dfa-9b9b-a1953352569f",
   "metadata": {},
   "source": [
    "### check the shape of the vectorized corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d566ad8-4d6f-489e-a3b2-f9535578ef44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9543, 6762)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_corpus_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1e6b42-3e3b-4812-902a-c83e19eca347",
   "metadata": {},
   "source": [
    "### check the vectorized corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2431b5a1-1b8c-4332-82e0-bfff1d721e0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_corpus_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852d5a0b-2afd-4990-aeb3-98bfcf740287",
   "metadata": {},
   "source": [
    "**Surely, it's a sparse matrix.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1e6e26-1e4a-4fbc-9d64-8bf10ec0d1d4",
   "metadata": {},
   "source": [
    "### check the fitted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e9b5be6-4700-4b22-85d2-69c1e359c2cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a320neo',\n",
       " 'a350',\n",
       " 'aal',\n",
       " 'aaoi',\n",
       " 'aapl',\n",
       " 'ab',\n",
       " 'abb',\n",
       " 'abbv',\n",
       " 'abbvie',\n",
       " 'abc']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_corpus_feature_names[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28685d2d-92fc-4e9d-a8ab-b9f4889f0fb3",
   "metadata": {},
   "source": [
    "### Count Vectorized matrix containing number of occurance of each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4370338e-50bc-4b4c-ae25-f6f0c05ad275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9543, 6762)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_corpus_feature_count.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40a1048f-58e0-4498-8f71-264574183e81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_corpus_feature_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c67fb80-265d-44db-8b84-047d3ce73b59",
   "metadata": {},
   "source": [
    "# (c) doc to vectors examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bbcde20-2b95-4945-bb69-fe0ac630efb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the corresponding vector for doc1 is  [[0. 0. 0. ... 0. 0. 0.]]\n",
      "the corresponding vector for doc2 is  [[0. 0. 0. ... 0. 0. 0.]]\n",
      "the corresponding vector for doc3 is  [[0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "doc1 = 'Jabberwocky'\n",
    "doc2 = 'buy $MSFT sell $AAPL hold Brent'\n",
    "doc3 = 'bullish #stocks'\n",
    "\n",
    "print(\"the corresponding vector for doc1 is \", Tfidf_vectorizer.transform([doc1]).toarray())\n",
    "print(\"the corresponding vector for doc2 is \", Tfidf_vectorizer.transform([doc2]).toarray())\n",
    "print(\"the corresponding vector for doc3 is \", Tfidf_vectorizer.transform([doc3]).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a1a7cc-83c5-4715-9ca8-0dad7e2c7140",
   "metadata": {},
   "source": [
    "### If you insist to know, uncomment the code below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "035d0a37-14a0-4951-a290-5b38e7005ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.set_printoptions(threshold=sys.maxsize)\n",
    "# print(\"the corresponding vector for doc1 is \", Tfidf_vectorizer.transform([doc1]).toarray())\n",
    "# print(\"the corresponding vector for doc2 is \", Tfidf_vectorizer.transform([doc2]).toarray())\n",
    "# print(\"the corresponding vector for doc3 is \", Tfidf_vectorizer.transform([doc3]).toarray())\n",
    "# np.set_printoptions(threshold=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ea34eb-bd97-4def-8050-3ac29044f975",
   "metadata": {
    "tags": []
   },
   "source": [
    "# (d) LSA Recommender Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af31dc09-1df0-4e03-b816-5f14846b5aea",
   "metadata": {},
   "source": [
    "**Here the question requests us to output the matrix containing lists of strings corresponding to the given str; however, the results are very ugly and hard to compare since every string has different length, so I decide to output the indices and print out the corresponding strings at the end.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9aceef52-ee2a-476c-93b0-af548b6bca96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_components=50, model=predict_KNeighbors(algorithm='brute', metric='l2')\n",
      "Indices Grid: \n",
      " [[ 687  622  682  511  650]\n",
      " [5034 4869 7334 8189 8085]\n",
      " [7874 9364 6446 2099 6524]\n",
      " [2604 2568 2675 2487 2674]]\n",
      "\n",
      "TOP 5 CLOEST strings for \"Fed\" are: \n",
      "\t1 The Fed is on hold for now, but it might not take much to change that\n",
      "\t2 LIVE: Fed Chair Jerome Powell speaks on today's decision to hold rates ▶️\n",
      "\n",
      "\n",
      "\t3 The Fed Faces a Housing Conundrum\n",
      "\t4 Chairman Jerome Powell speaks after Fed leaves interest rates unchanged \n",
      "\t5 Powell's 'half-full' U.S. glass sturdy but still at risk for spills as Fed meets\n",
      " \n",
      "\n",
      "TOP 5 CLOEST strings for \"$MSFT\" are: \n",
      "\t1 Riassunto: Priyanka Chopra Jonas e Crocs doneranno 50.000 paia di zoccoli classici all’UNICEF\n",
      "\t2 More later. \n",
      "\t3 Ruth Bader Ginsburg Hospitalized Again \n",
      "\t4 $EXPE continuation \n",
      "\t5 Signet Jewelers Finally Regains Some Sparkle\n",
      "\n",
      "TOP 5 CLOEST strings for \"tech rally\" are: \n",
      "\t1 $OPK EOD rally !!!\n",
      "\t2 Lyft and Peloton Rally, Defying the Mantle of 2019’s Least-Loved Debuts\n",
      "\t3 Auto suppliers rally on tariff relief hopes\n",
      "\t4 Buenos Aires Surrender Sends Argentine Bonds on a Wild Rally\n",
      "\t5 Italy’s Sardines Rally in Backlash Against League’s Salvini\n",
      "\n",
      "TOP 5 CLOEST strings for \"disappoint earnings\" are: \n",
      "\t1 $VREX - Varex Imaging Q1 2020 Earnings Preview \n",
      "\t2 $NTNX - Nutanix Q1 2020 Earnings Preview \n",
      "\t3 Earnings Preview: What To Expect From Macy’s On Thursday\n",
      "\t4 $AMCR - Amcor 1H 2020 Earnings Preview \n",
      "\t5 Earnings Preview: What To Expect From Home Depot On Tuesday\n",
      "  done in 0.332sec\n",
      "\n",
      "For n_components=200, model=predict_KNeighbors(algorithm='brute', metric='l2')\n",
      "Indices Grid: \n",
      " [[ 682  689  481  562  650]\n",
      " [3903 8448 8382 4077 4823]\n",
      " [7874 6446 9364 6524 2099]\n",
      " [9022 9008 2831 2487 2675]]\n",
      "\n",
      "TOP 5 CLOEST strings for \"Fed\" are: \n",
      "\t1 The Fed Faces a Housing Conundrum\n",
      "\t2 The Fed plans to lend directly to companies outside the banking system. Former Fed Governor Narayana Kocherlakota s… \n",
      "\t3 An emerging priority for Powell Fed: The plight of the poor  \n",
      "\t4 Fed: Powell stressed policy dependent on incoming information\n",
      "\t5 Powell's 'half-full' U.S. glass sturdy but still at risk for spills as Fed meets\n",
      " \n",
      "\n",
      "TOP 5 CLOEST strings for \"$MSFT\" are: \n",
      "\t1 \"They Have Stolen From All Saxons!\" - Thieves Steal 100s Of Priceless Jewels From Dresden Museum  \n",
      "\t2 DFND\n",
      "\t3 ACHV\n",
      "\t4 A Tokióban található teamLab Planets, a múzeum, ahol a víz a kiállítási közeg, idén tavasszal egy kis időre cseresznyevirág-szirmokba öltözik. A kiállítás 2020. március 1-től tekinthető meg.\n",
      "\t5 LTI Utökar Strategiskt Partnerskap med OKQ8 Scandinavia\n",
      "\n",
      "TOP 5 CLOEST strings for \"tech rally\" are: \n",
      "\t1 $OPK EOD rally !!!\n",
      "\t2 Auto suppliers rally on tariff relief hopes\n",
      "\t3 Lyft and Peloton Rally, Defying the Mantle of 2019’s Least-Loved Debuts\n",
      "\t4 Italy’s Sardines Rally in Backlash Against League’s Salvini\n",
      "\t5 Buenos Aires Surrender Sends Argentine Bonds on a Wild Rally\n",
      "\n",
      "TOP 5 CLOEST strings for \"disappoint earnings\" are: \n",
      "\t1 Ciena -2.5% after earnings miss\n",
      "\t2 Brink's down on earnings miss\n",
      "\t3 More on Worthington Q2 earnings\n",
      "\t4 $AMCR - Amcor 1H 2020 Earnings Preview \n",
      "\t5 Earnings Preview: What To Expect From Macy’s On Thursday\n",
      "  done in 1.373sec\n",
      "\n",
      "For n_components=500, model=predict_KNeighbors(algorithm='brute', metric='l2')\n",
      "Indices Grid: \n",
      " [[ 689  481  440 6318  646]\n",
      " [2014 9427 1858 8805 5158]\n",
      " [4037 7874 3871 9542 6524]\n",
      " [2670 2483 2865 8365 9500]]\n",
      "\n",
      "TOP 5 CLOEST strings for \"Fed\" are: \n",
      "\t1 The Fed plans to lend directly to companies outside the banking system. Former Fed Governor Narayana Kocherlakota s… \n",
      "\t2 An emerging priority for Powell Fed: The plight of the poor  \n",
      "\t3 $XLF $FAS $FAZ - Fed revives TALF to bolster ABS market \n",
      "\t4 Atlanta Fed hikes Q4 GDP estimate\n",
      "\t5 Patrick Harker, President of the Philly Fed, is due to speak now.\n",
      "\n",
      "TOP 5 CLOEST strings for \"$MSFT\" are: \n",
      "\t1 Vertex teams up with Molecular Templates in HSCT conditioning regimens\n",
      "\t2 TailDex up 8% and above 12.00. \n",
      "\t3 SASA聚酯选用英威达PTA技术\n",
      "\t4 SBFGP\n",
      "\t5 The 19 Dishes You Should Have Eaten in 2019\n",
      "\n",
      "TOP 5 CLOEST strings for \"tech rally\" are: \n",
      "\t1 A 2020 gift guide for the tech-savvy traveler \n",
      "\t2 $OPK EOD rally !!!\n",
      "\t3 Trade deal removes major hurdle for rally in Apple and tech\n",
      "\t4 YNDX, I, QD and OESX among tech movers\n",
      "\t5 Italy’s Sardines Rally in Backlash Against League’s Salvini\n",
      "\n",
      "TOP 5 CLOEST strings for \"disappoint earnings\" are: \n",
      "\t1 Do FDC's (NSE:FDC) Earnings Warrant Your Attention?\n",
      "\t2 Why Target Earnings Are Phenomenal\n",
      "\t3 Premium Earnings 2-10-20: Mercadolibre and Under Armour, on the blog and here \n",
      "\t4 6 Guru Stocks Expanding Earnings\n",
      "\t5 Cameco on the move after Q4 earnings topper\n",
      "  done in 4.880sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "# Rewrite the NearestNeighbors class to make it compatible with Pipepline and \n",
    "# introduce a predict function to the class\n",
    "class predict_KNeighbors(NearestNeighbors):\n",
    "    def transform(self, X=None, y=None):\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X=None, n_neighbors=5, return_distance=False):\n",
    "        return super(predict_KNeighbors, self).kneighbors(X, n_neighbors, return_distance)\n",
    "\n",
    "\n",
    "for n_components in [50, 200, 500]:\n",
    "    \n",
    "    # Construct Pipepline: vectorizer --> SVD --> KNN\n",
    "    lsarec = make_pipeline(\n",
    "        # tfidf_vectorizer\n",
    "        TfidfVectorizer(\n",
    "            max_df=0.5, # ignore terms which occur in more than half of the documents\n",
    "            max_features=10000,\n",
    "            min_df=2, # ignore terms which occur in less than 2 documents\n",
    "            stop_words='english',\n",
    "            norm='l2',\n",
    "            use_idf=True, \n",
    "            analyzer='word',\n",
    "            token_pattern = '(?u)\\\\b[a-zA-Z]\\\\w+\\\\b'\n",
    "        ),\n",
    "        # Project the tfidf vectors onto the first N principal components.\n",
    "        TruncatedSVD(\n",
    "            n_components=n_components,\n",
    "            random_state=42\n",
    "        #     algorithm='arpack'\n",
    "        ),\n",
    "        # Build a k-NN classifier. Use k = 5 (majority wins)\n",
    "        predict_KNeighbors(\n",
    "            n_neighbors=5, \n",
    "            algorithm='brute',\n",
    "            metric='l2'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # pipeline the whole corpus\n",
    "    full_corpus_lsa = lsarec.fit_transform(full_corpus)\n",
    "    \n",
    "    print(f'For n_components={n_components}, model={full_corpus_lsa}')\n",
    "    \n",
    "    # predict the 5 nearest neighbors of the given strings and output their indices\n",
    "    predicted_str_idx = lsarec.predict(['Fed', '$MSFT', 'tech rally', 'disappoint earnings'])\n",
    "    # print their indices\n",
    "    print(\"Indices Grid: \\n\", predicted_str_idx)\n",
    "    # print the 5 nearest stings for each input string\n",
    "    for i, text in enumerate(['Fed', '$MSFT', 'tech rally', 'disappoint earnings']):\n",
    "        print(f'\\nTOP 5 CLOEST strings for \"{text}\" are: ')\n",
    "        for j in range(5):\n",
    "            print(f'\\t{j+1}',full_corpus[predicted_str_idx[i][j]])\n",
    "            \n",
    "    print(\"  done in %.3fsec\\n\" % (time.time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebdfd00-0bee-473d-9510-a5e5bd4f786e",
   "metadata": {},
   "source": [
    "# (f) Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279b2874-b492-4349-abe7-b30dea5e3cbc",
   "metadata": {},
   "source": [
    "In latent semantic analysis (LSA), lemmatization refers to the process of reducing words to their base form, or lemma. This is done to group together words that have the same root but different inflections, such as \"walk,\" \"walking,\" and \"walked.\"\n",
    "\n",
    "Lemmatization is important in LSA because it helps to reduce the dimensionality of the text data by collapsing all inflected forms of a word into a single token. This can improve the accuracy of the analysis by reducing the noise caused by redundant or similar words.\n",
    "\n",
    "For example, without lemmatization, the words \"walk,\" \"walked,\" \"walking,\" and \"walks\" would be treated as separate words, even though they all have the same root. By lemmatizing these words to their base form \"walk,\" LSA can more accurately represent the underlying meaning of the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a318f4d-4016-4e17-a0ae-684ef90399a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/zhoukaiwen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"haven't\", 'yours', 'few', 'have', 'o', 'yourselves', 'himself', 'only', 'in', 'ourselves', 'needn', \"you'll\", 'into', 'ain', \"needn't\", 'what', 'which', \"doesn't\", 'shouldn', 'the', 'we', 'again', 'aren', 'hasn', 'if', 'some', 'after', 'between', 'how', 'very', 'more', 'wasn', 'during', 'hadn', 'yourself', \"shan't\", 'theirs', 'will', \"wasn't\", 'wouldn', 'itself', 'so', 'all', 'any', 'didn', 'below', 'them', 'he', 'nor', 'his', 'now', 'has', 'whom', 'it', 'hers', 'my', 'at', 'by', 'mustn', \"you'd\", 'do', 'no', 'this', \"it's\", 'off', 'am', 'doing', 'for', 'too', 'should', 'themselves', 's', 'and', \"you've\", 'under', 'me', 'because', 'where', \"you're\", 'y', \"won't\", 'doesn', 'about', 'your', 'as', 'were', 'does', 'just', \"aren't\", 'through', 'she', 'her', 'herself', 'from', 'there', 'won', 'that', 'our', 'its', 'further', 'with', 'having', 'haven', \"hadn't\", 'these', 'been', 'being', \"should've\", 'those', 'up', 'or', 'above', 't', \"weren't\", 'same', 'was', 'couldn', 'each', 'an', 'are', 're', \"that'll\", \"couldn't\", 'why', 'be', 'who', 'while', 'but', 'over', 'against', 'of', 'not', 'him', 'they', 'is', 'had', 'when', 'you', 'other', 'once', \"isn't\", 'isn', 'such', \"mustn't\", 'their', 'out', 'own', 'shan', 'to', \"hasn't\", 'than', \"didn't\", 'i', 'until', 'here', \"don't\", 'mightn', 'm', 'a', \"wouldn't\", 'don', 'll', 'before', 'ours', \"she's\", 'on', 'myself', 've', 'can', 'did', 'd', \"mightn't\", 'down', 'weren', 'most', 'both', \"shouldn't\", 'then', 'ma'}\n",
      "haven't yours few have o yourselves himself only in ourselves needn you'll into ain needn't what which doesn't shouldn the we again aren hasn if some after between how very more wasn during hadn yourself shan't theirs will wasn't wouldn itself so all any didn below them he nor his now has whom it hers my at by mustn you'd do no this it's off am doing for too should themselves s and you've under me because where you're y won't doesn about your as were does just aren't through she her herself from there won that our its further with having haven hadn't these been being should've those up or above t weren't same was couldn each an are re that'll couldn't why be who while but over against of not him they is had when you other once isn't isn such mustn't their out own shan to hasn't than didn't i until here don't mightn m a wouldn't don ll before ours she's on myself ve can did d mightn't down weren most both shouldn't then ma\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize    \n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "#nltk.download() # Used for downloading different stuff\n",
    "\n",
    "# Download stopwords list\n",
    "nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "# Interface lemma tokenizer from nltk with sklearn\n",
    "class LemmaTokenizer:\n",
    "    ignore_tokens = [',', '.', ';', ':', '\"', '``', \"''\", '`']\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc) if t not in self.ignore_tokens]\n",
    "    \n",
    "# Lemmatize the stop words\n",
    "tokenizer=LemmaTokenizer()\n",
    "print(stop_words)\n",
    "print(' '.join(stop_words))\n",
    "token_stop = tokenizer(' '.join(stop_words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff7d526-46df-45aa-a3a0-db874dcf03b1",
   "metadata": {},
   "source": [
    "### Incorporate the lemmatization into the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e736ab9c-f9fa-4d8b-b699-0b484e6b12a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_components=50, model=predict_KNeighbors(algorithm='brute', metric='l2')\n",
      "Indices Grid: \n",
      " [[ 622  511  481  621  562]\n",
      " [8160 8189 8153 8268 7954]\n",
      " [3677 2099 4233 6447 1536]\n",
      " [2674 2872 2846 2609 2817]]\n",
      "\n",
      "TOP 5 CLOEST strings for \"Fed\" are: \n",
      "\t1 LIVE: Fed Chair Jerome Powell speaks on today's decision to hold rates ▶️\n",
      "\n",
      "\n",
      "\t2 Chairman Jerome Powell speaks after Fed leaves interest rates unchanged \n",
      "\t3 An emerging priority for Powell Fed: The plight of the poor  \n",
      "\t4 LIVE: Fed Chair Jerome Powell delivers remarks at the decade's last FOMC meeting \n",
      "\t5 Fed: Powell stressed policy dependent on incoming information\n",
      "\n",
      "TOP 5 CLOEST strings for \"$MSFT\" are: \n",
      "\t1 $BIIB 😀 \n",
      "\t2 $EXPE continuation \n",
      "\t3 $APOP on scans.\n",
      "\t4 $SOXL \n",
      "\t5 All these names are working. Buying $BZH $HOV\n",
      "\n",
      "TOP 5 CLOEST strings for \"tech rally\" are: \n",
      "\t1 Sterling rallies as Tories unite behind Brexit plan\n",
      "\t2 Buenos Aires Surrender Sends Argentine Bonds on a Wild Rally\n",
      "\t3 Brexit Bulletin: The Cliff Edge is Back\n",
      "\t4 Boris Johnson Set to Back High-Speed Rail Project Despite Costs\n",
      "\t5 Hallmark Pulls Kissing Brides, and Netflix, DeGeneres Push Back\n",
      "\n",
      "TOP 5 CLOEST strings for \"disappoint earnings\" are: \n",
      "\t1 Earnings Preview: What To Expect From Home Depot On Tuesday\n",
      "\t2 Q1 Earnings Preview For Palo Alto Networks\n",
      "\t3 Nike earnings preview: what investors should watch\n",
      "\t4 A Preview Of Philip Morris Intl's Q4 Earnings\n",
      "\t5 Lattice Semiconductor Q4 Earnings Preview\n",
      "  done in 1.901sec\n",
      "\n",
      "For n_components=200, model=predict_KNeighbors(algorithm='brute', metric='l2')\n",
      "Indices Grid: \n",
      " [[ 481  562  682  689 6476]\n",
      " [8189 8268 8160 8153 8072]\n",
      " [6952 6446 7916 2099 4254]\n",
      " [2831 2595 2675 2472 2872]]\n",
      "\n",
      "TOP 5 CLOEST strings for \"Fed\" are: \n",
      "\t1 An emerging priority for Powell Fed: The plight of the poor  \n",
      "\t2 Fed: Powell stressed policy dependent on incoming information\n",
      "\t3 The Fed Faces a Housing Conundrum\n",
      "\t4 The Fed plans to lend directly to companies outside the banking system. Former Fed Governor Narayana Kocherlakota s… \n",
      "\t5 Economic Calendar Mon 11/25/19\n",
      "via Econoday\n",
      "\n",
      "Chicago Fed National Activity Index\n",
      "8:30 AM ET\n",
      "\n",
      "Dallas Fed Mfg Survey… \n",
      "\n",
      "TOP 5 CLOEST strings for \"$MSFT\" are: \n",
      "\t1 $EXPE continuation \n",
      "\t2 $SOXL \n",
      "\t3 $BIIB 😀 \n",
      "\t4 $APOP on scans.\n",
      "\t5 RECAP 4/7 +Pos Comments:\n",
      "$CLDR + Craig Hallum\n",
      "$XOM + CFRA\n",
      "$BTI + MS\n",
      "\n",
      "TOP 5 CLOEST strings for \"tech rally\" are: \n",
      "\t1 U.S. stocks are staging a remarkable two-day rally. But many analysts are calling the recent run a rally within a b… \n",
      "\t2 Auto suppliers rally on tariff relief hopes\n",
      "\t3 $TALKX: Market Briefing: Broad-based relief rally \n",
      "\t4 Buenos Aires Surrender Sends Argentine Bonds on a Wild Rally\n",
      "\t5 Can The Crypto Market Spur A Santa Claus Rally Following December Shivers?\n",
      "\n",
      "TOP 5 CLOEST strings for \"disappoint earnings\" are: \n",
      "\t1 More on Worthington Q2 earnings\n",
      "\t2 $SUMRX: Earnings Out This Afternoon/Tomorrow Morning \n",
      "\t3 Earnings Preview: What To Expect From Macy’s On Thursday\n",
      "\t4 Coca-Cola's Earnings: Outperforming Its Peers\n",
      "\t5 Q1 Earnings Preview For Palo Alto Networks\n",
      "  done in 4.153sec\n",
      "\n",
      "For n_components=500, model=predict_KNeighbors(algorithm='brute', metric='l2')\n",
      "Indices Grid: \n",
      " [[ 481  630  689  547  557]\n",
      " [8189 8268 8167 8237 5654]\n",
      " [3871 3859 7449 9542 2099]\n",
      " [2472 2865 2483 9008 2595]]\n",
      "\n",
      "TOP 5 CLOEST strings for \"Fed\" are: \n",
      "\t1 An emerging priority for Powell Fed: The plight of the poor  \n",
      "\t2 N.Y. FED'S 42-DAY REPO OP. OVERSUBSCRIBED; $49.05B OF BIDS\n",
      "\t3 The Fed plans to lend directly to companies outside the banking system. Former Fed Governor Narayana Kocherlakota s… \n",
      "\t4 Fed nominee Shelton gets bipartisan grilling in Senate \n",
      "\t5 Fed to beat a faster retreat from repo market \n",
      "\n",
      "TOP 5 CLOEST strings for \"$MSFT\" are: \n",
      "\t1 $EXPE continuation \n",
      "\t2 $SOXL \n",
      "\t3 $CEI Woke up\n",
      "\t4 $NUE 👀 \n",
      "\t5 $penn coin\n",
      "\n",
      "TOP 5 CLOEST strings for \"tech rally\" are: \n",
      "\t1 Trade deal removes major hurdle for rally in Apple and tech\n",
      "\t2 Tech Breakthrough Could Spark A Geothermal Energy Boom\n",
      "\t3 Venture capitalists, start-up founders and tech workers have warmed to Pete Buttigieg\n",
      "\n",
      "\t4 YNDX, I, QD and OESX among tech movers\n",
      "\t5 Buenos Aires Surrender Sends Argentine Bonds on a Wild Rally\n",
      "\n",
      "TOP 5 CLOEST strings for \"disappoint earnings\" are: \n",
      "\t1 Coca-Cola's Earnings: Outperforming Its Peers\n",
      "\t2 Premium Earnings 2-10-20: Mercadolibre and Under Armour, on the blog and here \n",
      "\t3 Why Target Earnings Are Phenomenal\n",
      "\t4 Brink's down on earnings miss\n",
      "\t5 $SUMRX: Earnings Out This Afternoon/Tomorrow Morning \n",
      "  done in 9.361sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "for n_components in [50, 200, 500]:\n",
    "    # Construct Pipepline: vectorizer --> SVD --> KNN\n",
    "    lemm_lsarec = make_pipeline(\n",
    "        # tfidf_vectorizer incorporated lemmatized tokenizer\n",
    "        TfidfVectorizer(\n",
    "            max_df=0.5, # ignore terms which occur in more than half of the documents\n",
    "            max_features=10000,\n",
    "            min_df=2, # ignore terms which occur in less than 2 documents\n",
    "            stop_words=token_stop,\n",
    "            norm='l2',\n",
    "            use_idf=True, \n",
    "            analyzer='word',\n",
    "            token_pattern = '(?u)\\\\b[a-zA-Z]\\\\w+\\\\b',\n",
    "            tokenizer=tokenizer\n",
    "        ),\n",
    "        # Project the tfidf vectors onto the first N principal components.\n",
    "        TruncatedSVD(\n",
    "            n_components=n_components,\n",
    "            random_state=42\n",
    "        #     algorithm='arpack'\n",
    "        ),\n",
    "        # Build a k-NN classifier. Use k = 5 (majority wins)\n",
    "        predict_KNeighbors(\n",
    "            n_neighbors=5, \n",
    "            algorithm='brute', \n",
    "            metric='l2'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # pipeline the whole corpus\n",
    "    full_corpus_lemm_lsa = lemm_lsarec.fit_transform(full_corpus)\n",
    "    \n",
    "    print(f'For n_components={n_components}, model={full_corpus_lemm_lsa}')\n",
    "    # predict the 5 nearest neighbors of the given strings and output their indices\n",
    "    predicted_str_idx = lemm_lsarec.predict(['Fed', '$MSFT', 'tech rally', 'disappoint earnings'])\n",
    "    # print their indices\n",
    "    print(\"Indices Grid: \\n\", predicted_str_idx)\n",
    "    # print the 5 nearest stings for each input string\n",
    "    for i, text in enumerate(['Fed', '$MSFT', 'tech rally', 'disappoint earnings']):\n",
    "        print(f'\\nTOP 5 CLOEST strings for \"{text}\" are: ')\n",
    "        for j in range(5):\n",
    "            print(f'\\t{j+1}',full_corpus[predicted_str_idx[i][j]])\n",
    "            \n",
    "    print(\"  done in %.3fsec\\n\" % (time.time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462e86e4-dbdd-43f8-a34a-3ac47fd1765a",
   "metadata": {},
   "source": [
    "**We can see the results have changed.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66237797-4e7e-45cb-ad9f-a0be4c3ad8b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
